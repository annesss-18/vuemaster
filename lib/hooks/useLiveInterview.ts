'use client';

import { useCallback, useRef, useState, useEffect } from 'react';
import { GoogleGenAI, Modality, LiveServerMessage, Session } from '@google/genai';
import { logger } from '@/lib/logger';

export interface TranscriptEntry {
    role: 'user' | 'model';
    content: string;
    timestamp: number;
}

interface InterviewContext {
    role: string;
    companyName?: string;
    level?: string;
    type?: string;
    techStack?: string[];
    questions?: string[];
    resumeText?: string;
}

export type ConnectionStatus = 'idle' | 'connecting' | 'connected' | 'disconnected' | 'error';

interface UseLiveInterviewReturn {
    status: ConnectionStatus;
    error: string | null;
    transcript: TranscriptEntry[];
    isAIResponding: boolean;
    isUserSpeaking: boolean;
    currentCaption: string;
    currentSpeaker: 'user' | 'model' | null;
    elapsedTime: number;
    connect: () => Promise<void>;
    disconnect: () => void;
    sendAudio: (base64Data: string) => void;
    sendInitialPrompt: () => void;
    onAudioReceived: (callback: (base64Data: string) => void) => void;
}

interface UseLiveInterviewOptions {
    sessionId: string;
    interviewContext: InterviewContext;
    onInterruption?: () => void;
}

/**
 * Hook for managing Gemini Live API WebSocket connection for live interviews.
 */
export function useLiveInterview(options: UseLiveInterviewOptions): UseLiveInterviewReturn {
    const { sessionId, interviewContext, onInterruption } = options;

    const [status, setStatus] = useState<ConnectionStatus>('idle');
    const [error, setError] = useState<string | null>(null);
    const [transcript, setTranscript] = useState<TranscriptEntry[]>([]);
    const [isAIResponding, setIsAIResponding] = useState(false);
    const [elapsedTime, setElapsedTime] = useState(0);
    // Current caption for subtitle display (last spoken text)
    const [currentCaption, setCurrentCaption] = useState<string>('');
    const [currentSpeaker, setCurrentSpeaker] = useState<'user' | 'model' | null>(null);
    const [isUserSpeaking, setIsUserSpeaking] = useState(false);

    const sessionRef = useRef<Session | null>(null);
    const audioCallbackRef = useRef<((base64Data: string) => void) | null>(null);
    const timerIntervalRef = useRef<NodeJS.Timeout | null>(null);
    const currentTranscriptRef = useRef<string>('');  // Accumulates AI model text
    const userTranscriptRef = useRef<string>('');     // Accumulates user speech text
    const userTranscriptTimeoutRef = useRef<NodeJS.Timeout | null>(null);  // Debounce for user transcript
    const reconnectionAttemptsRef = useRef(0);
    const isIntentionalDisconnectRef = useRef(false);
    const isConnectedRef = useRef(false); // Synchronous connection status tracking

    // Timer effect
    useEffect(() => {
        if (status === 'connected') {
            timerIntervalRef.current = setInterval(() => {
                setElapsedTime(prev => prev + 1);
            }, 1000);
        } else {
            if (timerIntervalRef.current) {
                clearInterval(timerIntervalRef.current);
                timerIntervalRef.current = null;
            }
        }

        return () => {
            if (timerIntervalRef.current) {
                clearInterval(timerIntervalRef.current);
            }
        };
    }, [status]);

    // Send initial prompt when connection is established
    // This is in a useEffect because we need sessionRef.current to be set first
    const hasInitialPromptSentRef = useRef(false);
    useEffect(() => {
        if (status === 'connected' && sessionRef.current && !hasInitialPromptSentRef.current) {
            hasInitialPromptSentRef.current = true;
            logger.debug('ðŸ“¤ Sending initial prompt to start interview...');
            try {
                sessionRef.current.sendClientContent({
                    turns: [
                        {
                            role: 'user',
                            parts: [{ text: 'Hello, I am ready to begin the interview. Please introduce yourself and start the interview.' }],
                        },
                    ],
                    turnComplete: true,
                });
                logger.debug('âœ… Initial prompt sent successfully');
            } catch (err) {
                console.error('Failed to send initial prompt:', err);
            }
        }

        // Reset the flag when disconnected so prompt can be sent again on reconnect
        if (status === 'disconnected' || status === 'idle') {
            hasInitialPromptSentRef.current = false;
        }
    }, [status]);

    const handleMessage = useCallback((message: LiveServerMessage) => {
        // Handle interruption
        if (message.serverContent?.interrupted) {
            setIsAIResponding(false);
            setCurrentSpeaker(null);
            currentTranscriptRef.current = '';
            onInterruption?.();
            return;
        }

        // Handle model turn (audio response)
        if (message.serverContent?.modelTurn?.parts) {
            setIsAIResponding(true);
            setCurrentSpeaker('model');
            setIsUserSpeaking(false);

            for (const part of message.serverContent.modelTurn.parts) {
                // Audio data
                if (part.inlineData?.data) {
                    logger.debug('ðŸ“¢ Received audio chunk from Gemini, length:', part.inlineData.data.length);
                    if (audioCallbackRef.current) {
                        audioCallbackRef.current(part.inlineData.data);
                    } else {
                        console.warn('âš ï¸ No audio callback registered!');
                    }
                }

                // Text from modelTurn is internal thinking - store for transcript but don't display
                // The actual spoken text comes via outputTranscription
                if (part.text) {
                    currentTranscriptRef.current += part.text;
                }
            }
        }

        // Handle turn complete
        if (message.serverContent?.turnComplete) {
            setIsAIResponding(false);

            // Clear internal thinking buffer (not used for display)
            currentTranscriptRef.current = '';

            // Clear caption after a brief delay if not interrupted by user
            setTimeout(() => {
                setCurrentCaption('');
                setCurrentSpeaker(null);
            }, 2000);
        }

        // Handle input transcription (user speech)
        // Accumulate words and debounce to create complete sentences
        if (message.serverContent?.inputTranscription) {
            const userText = message.serverContent.inputTranscription.text;
            if (userText) {
                // Set user as current speaker
                setCurrentSpeaker('user');
                setIsUserSpeaking(true);

                // Accumulate the text
                userTranscriptRef.current += userText;

                // Update caption with current user speech
                setCurrentCaption(userTranscriptRef.current.trim());

                // Clear any existing timeout
                if (userTranscriptTimeoutRef.current) {
                    clearTimeout(userTranscriptTimeoutRef.current);
                }

                // Set a debounce timeout - add to transcript after 1.5 seconds of silence
                userTranscriptTimeoutRef.current = setTimeout(() => {
                    const accumulatedText = userTranscriptRef.current.trim();
                    if (accumulatedText) {
                        setTranscript(prev => [...prev, {
                            role: 'user',
                            content: accumulatedText,
                            timestamp: Date.now(),
                        }]);
                        userTranscriptRef.current = '';
                    }
                    // Clear user speaking state after debounce
                    setIsUserSpeaking(false);
                    setCurrentSpeaker(null);
                }, 1500);
            }
        }

        // Handle output transcription (actual spoken text from the model)
        // This is what was actually spoken, not internal thinking
        if (message.serverContent?.outputTranscription) {
            const modelText = message.serverContent.outputTranscription.text;
            if (modelText) {
                // Update caption with actual spoken text
                setCurrentCaption(prev => prev + modelText);
                setCurrentSpeaker('model');

                // Also update transcript
                setTranscript(prev => {
                    // Update last model entry if exists
                    const lastEntry = prev[prev.length - 1];
                    if (lastEntry?.role === 'model') {
                        return [
                            ...prev.slice(0, -1),
                            { ...lastEntry, content: lastEntry.content + modelText },
                        ];
                    }
                    return [...prev, {
                        role: 'model',
                        content: modelText,
                        timestamp: Date.now(),
                    }];
                });
            }
        }
    }, [onInterruption]);

    const connect = useCallback(async () => {
        try {
            setStatus('connecting');
            setError(null);
            isIntentionalDisconnectRef.current = false;

            // 1. Get ephemeral token from our API
            const tokenResponse = await fetch('/api/live/token', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    sessionId,
                    interviewContext,
                }),
            });

            if (!tokenResponse.ok) {
                const errorData = await tokenResponse.json();
                throw new Error(errorData.error || 'Failed to get authentication token');
            }

            const { token, model } = await tokenResponse.json();

            // 2. Create GenAI client with ephemeral token
            // Ephemeral tokens require v1alpha API version
            const ai = new GoogleGenAI({
                apiKey: token,
                httpOptions: { apiVersion: 'v1alpha' }
            });

            // 3. Connect to Live API
            const session = await ai.live.connect({
                model: model,
                config: {
                    responseModalities: [Modality.AUDIO, Modality.TEXT],
                    speechConfig: {
                        languageCode: 'en-US',
                    },
                },
                callbacks: {
                    onopen: () => {
                        logger.info('Live API connection established');
                        isConnectedRef.current = true;
                        setStatus('connected');
                        reconnectionAttemptsRef.current = 0;
                    },
                    onmessage: (message: LiveServerMessage) => {
                        handleMessage(message);
                    },
                    onerror: (e: ErrorEvent) => {
                        console.error('Live API error:', e);
                        isConnectedRef.current = false;
                        setError(e.message || 'Unknown WebSocket error');
                        setStatus('error');
                    },
                    onclose: (e: CloseEvent) => {
                        logger.info('Live API connection closed:', e.reason);
                        isConnectedRef.current = false;
                        if (!isIntentionalDisconnectRef.current) {
                            setStatus('disconnected');
                            reconnect();
                        } else {
                            setStatus('disconnected');
                        }
                    },
                },
            });

            sessionRef.current = session;

        } catch (err) {
            const errorMessage = err instanceof Error ? err.message : 'Connection failed';
            setError(errorMessage);
            setStatus('error');
            throw err;
        }
    }, [sessionId, interviewContext, handleMessage]); // Added handleMessage dependency

    const reconnect = useCallback(async () => {
        const maxReconnectionAttempts = 5;
        const baseDelay = 1000;

        if (reconnectionAttemptsRef.current >= maxReconnectionAttempts) {
            setError('Failed to reconnect after multiple attempts');
            setStatus('error');
            return;
        }

        const delay = baseDelay * Math.pow(2, reconnectionAttemptsRef.current);
        reconnectionAttemptsRef.current += 1;

        logger.info(`Attempting to reconnect in ${delay}ms (attempt ${reconnectionAttemptsRef.current}/${maxReconnectionAttempts})`);

        await new Promise(resolve => setTimeout(resolve, delay));

        try {
            await connect();
        } catch (error) {
            console.error('Reconnection failed:', error);
            // Recursive call is safe here because of the async delay
            await reconnect();
        }
    }, [connect]);


    const disconnect = useCallback(() => {
        isIntentionalDisconnectRef.current = true;
        isConnectedRef.current = false;
        if (sessionRef.current) {
            sessionRef.current.close();
            sessionRef.current = null;
        }
        setStatus('disconnected');
    }, []);

    const sendAudio = useCallback((base64Data: string) => {
        if (!sessionRef.current) {
            // Session not available, this is normal during initial connection
            return;
        }

        // Use ref for synchronous status check (state updates are async)
        if (!isConnectedRef.current) {
            // Not connected yet, silently skip
            return;
        }

        try {
            sessionRef.current.sendRealtimeInput({
                audio: {
                    data: base64Data,
                    mimeType: 'audio/pcm;rate=16000',
                },
            });
        } catch (error) {
            console.error('Failed to send audio chunk:', error);
        }
    }, []);

    // Send an initial text prompt to trigger the AI to start speaking
    // Note: This is now called automatically in the onopen callback
    const sendInitialPrompt = useCallback(() => {
        if (!sessionRef.current || !isConnectedRef.current) {
            console.warn('âš ï¸ Cannot send prompt: not connected');
            return;
        }

        try {
            logger.debug('ðŸ“¤ Sending prompt...');
            sessionRef.current.sendClientContent({
                turns: [
                    {
                        role: 'user',
                        parts: [{ text: 'Hello, I am ready to begin the interview. Please introduce yourself and start the interview.' }],
                    },
                ],
                turnComplete: true,
            });
        } catch (error) {
            console.error('Failed to send prompt:', error);
        }
    }, []);

    const onAudioReceived = useCallback((callback: (base64Data: string) => void) => {
        audioCallbackRef.current = callback;
    }, []);

    // Cleanup on unmount
    useEffect(() => {
        return () => {
            disconnect();
        };
    }, [disconnect]);

    return {
        status,
        error,
        transcript,
        isAIResponding,
        isUserSpeaking,
        currentCaption,
        currentSpeaker,
        elapsedTime,
        connect,
        disconnect,
        sendAudio,
        sendInitialPrompt,
        onAudioReceived,
    };
}
